<section class="grey-section">
  

    <div class="col-lg-12">
        <h1 class="sub-section-header" id="one"> <mark>1: A Weird Time to be Enthusiastic about Computers</mark> <br><br>12/20/23</h1>
    </div>

    <div class="col-lg-12">
      <h4 style="text-align: left;"><code>I want to preface this with the fact that Artificial Intelligence will not be a common topic in future blog posts, as my interests are in other areas of computing. 
I just wanted to share my perspective as someone who has more insight than the average person. 
I am by no means an expert on Artificial Intelligence.</code></h4>
      <p> 2023 has been a weird year for me as a technology enthusiast. 
I have seen a lot of controversy online about generative AI. 
The main thing I want to do in this blog post is do my best as an enthusiast, and someone who has done their best to educate themselves on the topic, to provide as much context as possible to my friends and peers, who may be less inclined to spend hours a day reading articles and watching panels from the top experts, and mainly rely on social media for their information. 
Firstly, I would like to make it clear that there are absolutely problems with Artificial Intelligence right now. 
The most common complaint I have seen presented by my friends is the use of generative AI to create Artificial Intelligence art either in written form, or to make digital artwork, like paintings, or music. 
This is often delegated to “annoying” in the same vein as NFTs, but there is also a lot of talk about how Artificial Intelligence can be used maliciously for deep fakes, to spread misinformation or propaganda. 
I would 100% agree that these are real problems that need to be addressed, but, I do not have good answers for those problems, and I don’t know if anyone does. 
Many would argue that these problems should have been considered before these models were released to the public, or that companies should have waited until regulation had passed before publishing them. 
While these are valid arguments that I agree with, I do have some understanding of why this did not happen:

      <br><br>

        The first problem that comes to mind is the fact that ChatGPT was developed in a country with strong capitalist ideals. 
While OpenAI started as a non-profit organization, that doesn't necessarily mean they have no incentive to turn a profit, and since their inception, it has since established a “capped-profit” arm. 
While OpenAI's business structure is complex, having the company acquired would prove profitable for its board and beneficial for its employees no matter what. 
Additionally, from its inception, OpenAI's mission has been to "ensure that artificial intelligence benefits all of humanity", and to make that statement true, they would need large investments to support the development of their products. 
Just because they are a non-profit company does not mean that they do not need money.

<br><br>
        
        Another consideration that has come up online in more dedicated tech communities is that OpenAI wanted to be the first to release their model, so that they could have the largest influence over how regulation on Large Language Models is implemented and enforced. 
This is more of a "long-game play" in the sense that OpenAI could try to encourage regulation that would require exceptionally high standards for new companies, with the intention of reducing competition in the market from startups or open source options. 
Obviously this gives them the advantage of market share, which is a main point of concern for potential investors. 
Less competition also means that it would be harder for users to escape OpenAI’s "walled garden". 
In this case, that would mean that users would be stuck using OpenAI's product because they rely on ChatGPT's availability, bandwidth, selection of plugins, or just the amount of data they have access to train their model on. 
Startup companies would struggle to compete if they were spending most of their time trying to comply with regulations. 
While this started as a joke, and slowly turned into a bit of a conspiracy, it has turned out to be true to a certain extent: <a href=" https://iot-analytics.com/leading-generative-ai-companies/" class="link" target="_blank"> “Understanding who is ahead, who is upcoming, and the strengths and weaknesses of individual players and their models is important when building generative AI solutions and making important vendor and architecture decisions.” </a>
        
<br><br>

        This walled garden approach is popular for companies in new sectors because it can induce a snowball effect in customers - once you start using their service it can be hard to switch to another platform because of differing features, or even a less familiar interface (think about Apple, and the release of the iPhone). 
On top of this, it would allow companies such as Microsoft and Google to collect increasing amounts of data to further train and fine-tune their models, which competitors would not have access to.
        
</p><br><br><p>

        <a href="https://youtu.be/zNRUaLG2Mmw?si=PdKXdWYOu4Jxvrn9&t=2763" class="link" target="_blank">Experts</a> say that Artificial Intelligence needs to be open sourced to encourage its development into an open-source tool that can be used for the benefit of all humanity, like the computer, rather than being a piece of proprietary software owned and distributed by a select few megacorps. 

<br><br>

        A big advantage of open-sourcing models is that it would become clear what training data was being used, which would go a long way to making artists and writers happy. 
An open source dataset could be a big incentive for customers to choose startup companies' platforms over the offerings from corporations, with potential features like “not selling your data after we collect it and use it to improve our model”, which would definitely get me on board. 
Additionally, it would make it clear if there was any hard coded political or ethical bias, which is hard to prove for certain on private models. 
It would also allow the models to be more easily tailored to specific users needs, by giving developers the ability to modify both the training data set and the source code.

<br><br>

        An open-source nature would enable features such as generation signing, basically watermarking the metadata of the content that the model creates. 
This way, anything that is shared with malicious intent can be identified as such, either by the presence of the metadata watermark, or by the lack of a watermark altogether. 
Companies such as Adobe, Sony, and many others are already working on implementing this technology. 
While it is likely that OpenAI or Google would implement these features into their generative AI tools, it is also likely that they would do so on the assumption that they would be able to somehow turn a profit by doing so.
        </p>

        <h4>Now that's out of the way, I’ll share my opinions.</h4>

        <p>While there are real problems with how Artificial Intelligence is used in the public space, it is important to remember that its intended use case of generative AI is to be a very powerful tool. <a href="https://www.youtube.com/watch?v=3tS7HzWUbBs" class="link" target="_blank">Companies are already seeing its value, and implementing it into their procedures and products.</a> A couple good examples: 

          <ul>
            <li><a href="https://www.youtube.com/watch?v=n_-QuGvQXso" class="link" target="_blank">Cisco is incorporating Language Models into their newest firewalls, </a> allowing network engineers to interact with complex firewall rules without immediately getting a migraine. It also allows for rapid packet analysis, which can help block traffic from the increasing variety of attacks. It's a modern security feature that is basically required for modern security problems, as it would be impossible for humans to keep up with the number of new attacks being developed by bad actors with access to their own models.</li>
            <li>Adobe is implementing GenAI into photoshop to support features like generative fill.</li>
            <li><a href="https://gizmodo.com.au/2023/10/google-assistant-to-integrate-bard-chatbot-and-all-the-other-ai-features-coming-to-the-pixel-8/" class="link" target="_blank">Google </a> and <a href="https://arstechnica.com/apple/2023/12/apple-wants-ai-to-run-directly-on-its-hardware-instead-of-in-the-cloud/" class="link" target="_blank"> Apple </a> are both incorporating models into their personal assistants to greatly increase their abilities to carry a conversation and complete tasks.</li>
            <li>Startups like <a href="https://release.ai/" class="link" target="_blank">Release</a> are getting attention from investors by developing software that allows all kinds of people to perform their jobs faster with the help of language models.</li>
            <li>Various media platforms are implementing AI models to moderate content.</li>
            <li>Retail platforms are already beginning to implement LLMs as a first point of contact for customer service, <a href="https://x.com/ChrisJBakke/status/1736533308849443121?s=20" class="link" target="_blank">as seen here.</a></li>              
        </ul>
        <p>
          Based on the fact that major corporations are already beginning to take full advantage of this brand new technology, despite the public generally having a disapproving attitude for it, shows that it is going to quickly become a huge part of how we interact with technology in the coming decade whether we like it or not.
           Bill Gates even believes that <a href="https://www.linkedin.com/posts/williamhgates_if-i-had-to-make-a-prediction-i-would-guess-activity-7143302529546600448-TEj0/?utm_source=share&utm_medium=member_android" class="link" target="_blank"> interacting with Artificial Intelligence models will become a daily occurrence for Americans within the next 2 years.</a><br>
             A recent study from Microsoft Reaseach's <a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/NFWReport2023-6580ebba7b89b.pdf" class="link" target="_blank">New
            Future of Work Report</a> has some really good information related to this as well.
          <br><br>
          Lastly, I want to bring up the use of Artificial Intelligence in education. Currently, students using Artificial Intelligence are considered cheating.
           This is absolutely valid if students are using it to complete assignments on their behalf, however I think it is also hugely important for the united states education system to realize how important of a tool this will be for students in their adult life, in the same way that having access to and familiarity with computers has been hugely beneficial to older generations.
           While it will undoubtably be difficult to moderate, I believe it is imperative that Schools incorporate the use of these models in their curriculums. There was well over a decade where students had access to web browsers before schools had real control over what students could access with them, and even now these methods are generally not too difficult to circumvent.
           <br><br>
           I have personally learned so much from using GPT as my personal tutor, to answer questions, give examples, and fill in mental gaps while studying. (Current models are fully capable of helping with basically any technical tasks, as they have been trained on documentation and guides for all major platforms. This goes for K-12 School topics as well.) Speaking from personal experience, working with Artificial Intelligence models gives you the advantage of being able to ask questions that you may feel uncomfortable asking another person. For example, if you are in a class or lecture and a term or subject comes up that you aren’t familiar with, but you are embarrassed about not knowing, or if you have a question regarding personal health. Asking a language model can get you to a good starting point, or provide sources to do your own research. Possibly the most important point regarding Artificial Intelligence in education is making sure that students are not only able to interact with models effectively, but also ethically. I would bet that a teenager having a bad day would be the only thing capable of mis-using generative AI more than a malicious nation state.
          <br><br>

          To close things out, I want to finalize my standpoint on this topic: Artificial Intelligence has a lot of problems in the consumer space, but being proficient at interacting with it is going to be a relevant skill for most technical jobs in the near future, and as such, it's important to have some familiarity with it. It is also my personal belief that people should be properly familiar with the technology before they make statements on its politics. Ideally, the legislation and regulation should be directed by the developers of the models, rather than the corporations and executives that pay their wages, or worse: Twitter.
          <br><br>

          <code>While Artificial Intelligence isn’t the main focus of my career goals or studies, I love to discuss any topics related to computing, and I’m sure people will have some thoughts they would like to share! I can be contacted via email at patrick@quam.computer or you can connect with me on linkedin! Future posts will be more correlated with my daily studies and other major developments in the computing sector.</code>
          
        </p>
    </div>

    <div class="col-lg-12 bottom-nav">

      
        <div>
        </div>
        <div></div>
  
      </div>
</section>